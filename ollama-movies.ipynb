{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-ollama langchain-community beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import requests\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama.com tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://ollama.com/library/llama3.2/tags\",\n",
    "    # max_depth=2,\n",
    "    # use_async=False,\n",
    "    # extractor=None,\n",
    "    # metadata_extractor=None,\n",
    "    # exclude_dirs=(),\n",
    "    # timeout=10,\n",
    "    # check_response_status=True,\n",
    "    # continue_on_failure=True,\n",
    "    # prevent_outside=True,\n",
    "    # base_url=None,\n",
    "    # ...\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "\n",
    "loader = RecursiveUrlLoader(\"https://ollama.com/library/llama3.2/tags\", extractor=bs4_extractor)\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_tags(tags_and_descriptions: str, model):\n",
    "    # Prompt para a análise de tags.\n",
    "    prompt = (\n",
    "        \"The following is content extracted from a document. Analyze and summarize the details \"\n",
    "        \"specifically focusing on models optimized for small sizes:\\n\\n\"\n",
    "    )\n",
    "    prompt += tags_and_descriptions\n",
    "    return model(prompt)\n",
    "\n",
    "def analyze_tags_in_docs(docs: List, llm, max_tokens: int = 1800):\n",
    "    responses = []\n",
    "    for doc in docs:\n",
    "        # Extrai o conteúdo limitado ao máximo permitido.\n",
    "        limited_content = doc.page_content[:max_tokens]\n",
    "        # Analisa os tags no conteúdo extraído usando a LLM.\n",
    "        response = analyze_tags(limited_content, llm)\n",
    "        responses.append(response)\n",
    "    # Retorna as respostas finais unificadas.\n",
    "    final_response = \"\\n\".join(responses)\n",
    "    return final_response\n",
    "\n",
    "# Exemplo de chamada da função\n",
    "final_response = analyze_tags_in_docs(docs, llm)\n",
    "print(final_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Conta o número de tokens no texto usando o encoding especificado.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text_to_tokens(text: str, max_tokens: int, encoding_name: str = \"cl100k_base\") -> str:\n",
    "    \"\"\"\n",
    "    Trunca o texto para não ultrapassar o número máximo de tokens.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return encoding.decode(truncated_tokens)\n",
    "\n",
    "def analyze_tags(tags_and_descriptions: str, model):\n",
    "    \"\"\"\n",
    "    Análise de tags usando o modelo LLM.\n",
    "    \"\"\"\n",
    "    # Prompt para a análise de tags.\n",
    "    prompt = (\n",
    "        \"The following is content extracted from a document. Analyze and summarize the details \"\n",
    "        \"specifically focusing on models optimized for small sizes:\\n\\n\"\n",
    "    )\n",
    "    prompt += tags_and_descriptions\n",
    "    return model(prompt)\n",
    "\n",
    "def analyze_tags_in_docs(docs: List, llm, max_tokens: int = 4096, encoding_name: str = \"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Analisa tags nos documentos sem ultrapassar a janela de contexto.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    # Reservar tokens para o prompt fixo.\n",
    "    prompt_reservation = count_tokens(\n",
    "        \"The following is content extracted from a document. Analyze and summarize the details \"\n",
    "        \"specifically focusing on models optimized for small sizes:\\n\\n\",\n",
    "        encoding_name\n",
    "    )\n",
    "    allowed_tokens = max_tokens - prompt_reservation\n",
    "\n",
    "    for doc in docs:\n",
    "        # Trunca o conteúdo do documento para caber na janela de contexto.\n",
    "        limited_content = truncate_text_to_tokens(doc.page_content, allowed_tokens, encoding_name)\n",
    "        # Analisa as tags no conteúdo truncado usando a LLM.\n",
    "        response = analyze_tags(limited_content, llm)\n",
    "        responses.append(response)\n",
    "    \n",
    "    # Retorna as respostas finais unificadas.\n",
    "    final_response = \"\\n\".join(responses)\n",
    "    return final_response\n",
    "\n",
    "# Exemplo de chamada da função\n",
    "# Certifique-se de que `docs` é uma lista de objetos com um atributo `page_content`.\n",
    "# `llm` deve ser um objeto de um modelo LangChain LLM configurado.\n",
    "final_response = analyze_tags_in_docs(docs, llm)\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "\n",
    "loader = RecursiveUrlLoader(\"https://bflixhd.lol/home/\", extractor=bs4_extractor)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def truncate_text_to_tokens(text: str, max_tokens: int, encoding_name: str = \"cl100k_base\") -> str:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return encoding.decode(truncated_tokens)\n",
    "\n",
    "\n",
    "def analyze_tags(tags_and_descriptions: str, model, base_prompt: str) -> str:\n",
    "    prompt = base_prompt + tags_and_descriptions\n",
    "    return model(prompt)\n",
    "\n",
    "\n",
    "def analyze_tags_in_docs(docs: List, llm, max_tokens: int = 4096, encoding_name: str = \"cl100k_base\") -> str:\n",
    "    responses = []\n",
    "    base_prompt = (\n",
    "        \"Você é especialista em análise de filmes.\\n\"\n",
    "        \"Analise o conteúdo abaixo,entenda e extraia detalhes relevantes conforme exabaixo.\\n\"\n",
    "        \"ex: nome: Moana 2  nota: 7.2 ano: 2024 tempo: 100 min \"\n",
    "        \"Por favor, me dê apenas as informações principais sobre o site, sem avisos sobre segurança ou legalidade.\"\n",
    "        \"Gostaria de uma resposta curta e objetiva, sem explicações adicionais.\"\n",
    "        \"Entenda os dados e me dê um resumo bem pequeno dos dados conforme exemplo.\"\n",
    "        \"Dados:\\n\\n\"\n",
    "    )\n",
    "    prompt_reservation = count_tokens(base_prompt, encoding_name)\n",
    "    allowed_tokens = max_tokens - prompt_reservation\n",
    "\n",
    "    for doc in docs:\n",
    "        if not hasattr(doc, 'page_content'):\n",
    "            raise ValueError(\n",
    "                \"Cada item em 'docs' deve conter o atributo 'page_content'.\")\n",
    "\n",
    "        limited_content = truncate_text_to_tokens(\n",
    "            doc.page_content, allowed_tokens, encoding_name)\n",
    "        response = analyze_tags(limited_content, llm, base_prompt)\n",
    "        responses.append(response)\n",
    "\n",
    "    return \"\\n\".join(responses)\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "\n",
    "\n",
    "result = analyze_tags_in_docs(docs, llm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados de uma URL\n",
    "def load_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Extrair texto visível da página\n",
    "    return soup.get_text()\n",
    "\n",
    "# Função para dividir os dados em chunks com base na contagem de tokens\n",
    "def split_into_chunks(data, max_tokens, tokenizer):\n",
    "    tokens = tokenizer.encode(data)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurações\n",
    "url = \"https://bflixhd.lol/the-judge-from-hell/\"  # Substitua pelo URL desejado\n",
    "question = \"nome dos atores, origem do filme, ano de lançamento, sinopse\" \n",
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0.9)\n",
    "MAX_CHUNK_TOKENS = 4048  # Tamanho máximo permitido para o chunk\n",
    "\n",
    "# Carregar e dividir os dados\n",
    "try:\n",
    "    raw_data = load_data_from_url(url)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    chunks = split_into_chunks(raw_data, MAX_CHUNK_TOKENS, tokenizer)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar ou processar os dados: {e}\")\n",
    "    chunks = []\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Criar o prompt template\n",
    "template = \"\"\"\n",
    "{question}\n",
    "{data}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Processar cada chunk e acumular as respostas\n",
    "responses = []\n",
    "token_counts = []\n",
    "print(\"numero de  tokens por chunk\", len(chunks))\n",
    "for chunk in chunks:\n",
    "    token_count = len(tokenizer.encode(chunk))\n",
    "    print(\"token_count\", token_count)\n",
    "    token_counts.append(token_count)\n",
    "    print(\"token_counts.append\", token_counts)\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"question\": question, \"data\": chunk})\n",
    "    responses.append(response)\n",
    "\n",
    "# Combinar as respostas em uma resposta final\n",
    "final_answer = \" \".join(responses)\n",
    "\n",
    "# Exibir a resposta final e a contagem de tokens\n",
    "print(\"LLM Final Answer:\")\n",
    "print(final_answer)\n",
    "print(\"\\nToken Counts per Chunk:\")\n",
    "print(token_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "\n",
    "# Configurações\n",
    "url = \"https://bflixhd.lol/filters/\"  # Substitua pelo URL desejado\n",
    "question = \"nome do filme, ano de lançamento, e Scores, votes ou numero de views e tambem a sinopse\" \n",
    "few_shot = '''\n",
    "Considere os seguintes dados sobre filmes e séries. Extraia as informações de cada item e estruture-as de forma organizada como um JSON. Cada item deve conter os seguintes campos:\n",
    "- type: Tipo do conteúdo (ex: \"Movie\", \"Series\").\n",
    "- title: Título do conteúdo.\n",
    "- season: Número da temporada (se aplicável).\n",
    "- episode: Número do episódio (se aplicável).\n",
    "- year: Ano de lançamento.\n",
    "- rating: Avaliação do conteúdo (nota).\n",
    "- country: País de origem (se aplicável).\n",
    "- genre: Gêneros associados ao conteúdo.\n",
    "- duration: Duração do filme ou episódio (se aplicável).\n",
    "- score: A média das avaliações (se disponível).\n",
    "- reviews_count: Número de avaliações.\n",
    "- description: Descrição ou sinopse do conteúdo.\n",
    "\n",
    "Aqui estão os dados:\n",
    "\n",
    "HD    2024 SS 1 EP 15 Secret LevelSecret LevelHDTV-142024 7.9Country: USAGenre:Action, AnimationScores:7.9 by 14 reviews Adult animated series of original short stories which are set within the worlds of beloved video games. Each episode serves as a gateway to a new adventure, unlocking exciting worlds ...\n",
    "HD    2024 Movie 102 min Ebenezer the TravelerEbenezer the TravelerHDNR2024 7102 minGenre:FantasyScores:7 by 1 reviews Ebenezer the Traveler continues the journey of the famous Ebenezer Scrooge following the events of \"A Christmas Carol\". In this untold twist, Ebenezer Scrooge is now enlisted as one of ...\n",
    "HD    2024 Movie 91 min That ChristmasThat ChristmasHDPG2024 7.591 minCountry: UKGenre:Adventure, AnimationScores:7.5 by 54 reviews It\\'s an unforgettable Christmas for the townsfolk of Wellington-on-Sea when the worst snowstorm in history alters everyone\\'s plans — including Santa\\'s.\n",
    "HD    2024 Movie 110 min MaryMaryHDPG-132024 6.3110 minCountry: USAGenre:Action, DramaScores:6.3 by 27 reviews A miraculous conception. A merciless king. A murderous pursuit. Mary\\'s journey to give birth to Jesus unfolds in this biblical coming-of-age epic.\n",
    "HD    2024 SS 1 EP 2 Star Wars: Skeleton CrewStar Wars: Skeleton CrewHDTV-PG2024 7.2Country: USAGenre:Action, Science FictionScores:7.2 by 40 reviews Four ordinary kids search for their home planet after getting lost in the Star Wars galaxy.\n",
    "HD    2024 SS 1 EP 4 Tomorrow and ITomorrow and IHDTV-MA2024 8.2Country: ThailandGenre:Drama, Science FictionScores:8.2 by 6 reviews This series reimagines Thailand in a dystopian future where technology scrapes at the surface of old customs, exposing rips in the fabric of culture.\n",
    "HD    2024 SS 1 EP 6 Black DovesBlack DovesHDTV-MA2024 8Country: UKGenre:Action, CrimeScores:8 by 14 reviews When a spy posing as a politician\\'s wife learns her lover has been murdered, an old assassin friend joins her on a quest for truth — and vengeance.\n",
    "HD    2024 SS 1 EP 6 SennaSennaHDTV-MA2024 8.2Country: BrazilGenre:DramaScores:8.2 by 45 reviews Fascinated by cars since childhood, Brazilian racer Ayrton Senna became a sports legend — until tragedy struck, changing Formula 1 forever.\n",
    "CAM    2024 Movie 100 min Moana 2Moana 2CAMPG2024 7.2100 minCountry: CanadaGenre:Adventure, AnimationScores:7.2 by 94 reviews After receiving an unexpected call from her wayfinding ancestors, Moana journeys alongside Maui and a new crew to the far seas of Oceania and into dangerous, long-lost waters for an ...\n",
    "CAM    2024 Movie 161 min WickedWickedCAMPG2024 7.7161 minCountry: USAGenre:Drama, FantasyScores:7.7 by 255 reviews When ostracized and misunderstood green-skinned Elphaba is forced to share a room with the popular aristocrat Glinda.\n",
    "\n",
    "'''\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0.9)\n",
    "MAX_CHUNK_TOKENS = 4048  # Tamanho máximo permitido para o chunk\n",
    "\n",
    "# Carregar e dividir os dados\n",
    "try:\n",
    "    raw_data = load_data_from_url(url)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    chunks = split_into_chunks(raw_data, MAX_CHUNK_TOKENS, tokenizer)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar ou processar os dados: {e}\")\n",
    "    chunks = []\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Criar o prompt template\n",
    "template = \"\"\"\n",
    "{data}\n",
    "{few_shot}\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Processar cada chunk e acumular as respostas\n",
    "responses = []\n",
    "token_counts = []\n",
    "print(\"numero de  tokens por chunk\", len(chunks))\n",
    "for chunk in chunks:\n",
    "    token_count = len(tokenizer.encode(chunk))\n",
    "    print(\"token_count\", token_count)\n",
    "    token_counts.append(token_count)\n",
    "    print(\"token_counts.append\", token_counts)\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"few_shot\": few_shot,\n",
    "            \"data\": chunk\n",
    "        }\n",
    "    )\n",
    "    responses.append(response)\n",
    "\n",
    "# Combinar as respostas em uma resposta final\n",
    "final_answer = \" \".join(responses)\n",
    "\n",
    "# Exibir a resposta final e a contagem de tokens\n",
    "print(\"LLM Final Answer:\")\n",
    "print(final_answer)\n",
    "print(\"\\nToken Counts per Chunk:\")\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tempo de execução\n",
    "ollama\n",
    "7m\n",
    "\n",
    "llm studio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
