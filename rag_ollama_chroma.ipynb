{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "https://python.langchain.com/docs/integrations/providers/chroma/#retriever\n",
    "https://python.langchain.com/docs/integrations/text_embedding/\n",
    "https://python.langchain.com/docs/integrations/vectorstores/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/\n",
    "https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "https://python.langchain.com/docs/integrations/llms/ollama/\n",
    "https://python.langchain.com/docs/concepts/chat_models/\n",
    "https://python.langchain.com/docs/concepts/structured_outputs/\n",
    "https://python.langchain.com/docs/concepts/tokens/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/microsoft_word/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/image_captions/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/image/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/microsoft_excel/\n",
    "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/\n",
    "https://python.langchain.com/docs/integrations/memory/\n",
    "\n",
    "https://python.langchain.com/docs/additional_resources/youtube/#videos-sorted-by-views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-ollama pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import asyncio\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def generate_random_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "random_id = generate_random_id()\n",
    "random_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_standardize_file_path(file_path):\n",
    "    # Remove spaces and replace them with underscores\n",
    "    standardized_path = file_path.replace(\" \", \"_\")\n",
    "    return standardized_path\n",
    "file_path = fr\"file name with spaces.pdf\"\n",
    "path = rename_and_standardize_file_path(file_path)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:latest\",\n",
    ")\n",
    "# Asynchronously load pages and store metadata\n",
    "# Modify extract_pdf_pages to manually set page numbers\n",
    "async def extract_pdf_pages(loader, file_path, doc_id):\n",
    "    extracted_pages = []\n",
    "    page_number = 1\n",
    "\n",
    "    async for page in loader.alazy_load():  # Se der erro, use loader.load()\n",
    "        extracted_pages.append({\n",
    "            \"page_content\": page.page_content,\n",
    "            \"metadata\": {\n",
    "                \"source\": file_path,\n",
    "                \"page_number\": page_number,\n",
    "                \"doc_id\": doc_id\n",
    "            }\n",
    "        })\n",
    "        page_number += 1  # Incrementa manualmente o número da página\n",
    "\n",
    "    return extracted_pages\n",
    "\n",
    "file_name = \"file.pdf\"\n",
    "file_dir = r\"jupter_notebooks/\"\n",
    "file_path = os.path.join(file_dir, file_name)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"file not found: {file_path}\")\n",
    " \n",
    "async def indexer():\n",
    "    doc_id = \"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    \n",
    "    pages = await extract_pdf_pages(loader, file_path, doc_id)\n",
    "\n",
    "    texts = [page[\"page_content\"] for page in pages]\n",
    "    metadata = [page[\"metadata\"] for page in pages]\n",
    "\n",
    "    return texts, metadata\n",
    "\n",
    "texts, metadata = await indexer()\n",
    "\n",
    "print(texts)\n",
    "print(metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrival/vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "\n",
    "doc_id = ''\n",
    "question = f\"tell me about document id {doc_id}\"\n",
    "\n",
    "relevant_docs = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(question), k=3,filter={\"doc_id\": doc_id} \n",
    ")\n",
    "for doc in relevant_docs:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm/ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"deepseek-coder-v2:16b\"\n",
    "\n",
    "llm = OllamaLLM(model=model, temperature=0.7)\n",
    "\n",
    "retrieved_context = \"\\n\".join(\n",
    "    [\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page_number', 'Unknown')}\\n{doc.page_content}\"\n",
    "        for doc in relevant_docs\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the robust prompt for the LLM\n",
    "prompt = f\"\"\"\n",
    "You are a precise and skilled assistant. Your goal is to answer the user's question strictly based on the provided context.\n",
    "If the answer is not found in the context, respond with: \"The answer was not found in the provided context.\"\n",
    "\n",
    "Below is the context extracted from documents related to the query. For each part of your response, clearly indicate the source and the page number where the information was found.\n",
    "\n",
    "Your task:\n",
    "- Understand the user's question and rewrite the question's intent.\n",
    "- Review the context and answer the question based on the provided information.\n",
    "- Respond concisely, using only the context.\n",
    "- For each piece of information provided, mention the source and the page number.\n",
    "- If the answer cannot be found in the context, state this explicitly.\n",
    "- If the answer is not found, analyze again and attempt to respond.\n",
    "\n",
    "Question: {question}  \n",
    "Context: {retrieved_context}  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Get the LLM response\n",
    "response = llm(prompt)\n",
    "\n",
    "print(\"LLM response:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
