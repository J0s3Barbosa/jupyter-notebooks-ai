{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU langchain-community beautifulsoup4\n",
    "%pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2024-12-15 21:12:44  [INFO] [LM STUDIO SERVER] Supported endpoints:\n",
    "2024-12-15 21:12:44  [INFO] [LM STUDIO SERVER] ->\tGET  http://localhost:1234/v1/models\n",
    "2024-12-15 21:12:44  [INFO] [LM STUDIO SERVER] ->\tPOST http://localhost:1234/v1/chat/completions\n",
    "2024-12-15 21:12:44  [INFO] [LM STUDIO SERVER] ->\tPOST http://localhost:1234/v1/completions\n",
    "2024-12-15 21:12:44  [INFO] [LM STUDIO SERVER] ->\tPOST http://localhost:1234/v1/embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# URL do servidor LM Studio\n",
    "lm_studio_url = \"http://localhost:1234/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RecursiveUrlLoader(\n",
    "    \"https://ollama.com/library/llama3.2/tags\",\n",
    "    # max_depth=2,\n",
    "    # use_async=False,\n",
    "    # extractor=None,\n",
    "    # metadata_extractor=None,\n",
    "    # exclude_dirs=(),\n",
    "    # timeout=10,\n",
    "    # check_response_status=True,\n",
    "    # continue_on_failure=True,\n",
    "    # prevent_outside=True,\n",
    "    # base_url=None,\n",
    "    # ...\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "loader = RecursiveUrlLoader(\"https://bflixhd.lol/home/\", extractor=bs4_extractor)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados de uma URL\n",
    "def load_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Extrair texto visível da página\n",
    "    return soup.get_text()\n",
    "\n",
    "# Função para dividir os dados em chunks com base na contagem de tokens\n",
    "def split_into_chunks(data, max_tokens, tokenizer):\n",
    "    tokens = tokenizer.encode(data)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "\n",
    "# Configurações\n",
    "url = \"https://bflixhd.lol/filters\"  # Substitua pelo URL desejado\n",
    "# question = \"nome do filme, ano de lançamento, e Scores, votes ou numero de views e tambem a sinpse\" \n",
    "question = '''\n",
    "analise o no contexto dado.\n",
    "quantos filmes e séries tem no contexto e quais sao eles?\n",
    "Extraia os nomes de todos os filmes e series e seus scores.\n",
    "me de tambem o total de filmes e series.  \n",
    "'''\n",
    "\n",
    "MAX_CHUNK_TOKENS = 4096  # Tamanho máximo permitido para o chunk\n",
    "\n",
    "# Carregar e dividir os dados\n",
    "try:\n",
    "    raw_data = load_data_from_url(url)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    chunks = split_into_chunks(raw_data, MAX_CHUNK_TOKENS, tokenizer)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar ou processar os dados: {e}\")\n",
    "    chunks = []\n",
    "print(chunks)\n",
    "print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL para o endpoint de completions\n",
    "completions_url = f\"{lm_studio_url}/chat/completions\"\n",
    "    \n",
    "# Prompt base para o modelo\n",
    "prompt_template = \"{data}\\n\\n{question}\"\n",
    "\n",
    "model = \"mistral-7b-instruct-v0.3\"\n",
    "\n",
    "# Usar uma sessão do requests para melhorar a performance em múltiplas requisições\n",
    "with requests.Session() as session:\n",
    "    # Armazenar respostas e contagem de tokens\n",
    "    responses = []\n",
    "    token_counts = []\n",
    "    \n",
    "    print(f\"Número de tokens por chunk: {len(chunks)}\")\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        token_count = len(tokenizer.encode(chunk))\n",
    "        print(token_count)\n",
    "        token_counts.append(token_count)\n",
    "        print(token_counts)\n",
    "        prompt = prompt_template.format(data=chunk, question=question)\n",
    "\n",
    "        # Preparar a requisição POST\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"assistant\", \"content\": \"Você é um assistente útil.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Requisição POST\n",
    "            response = session.post(completions_url, json=payload)\n",
    "\n",
    "            # Verificar se a resposta foi bem-sucedida\n",
    "            response.raise_for_status()  # Levanta exceção para erros HTTP\n",
    "            content_response = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "            \n",
    "            if content_response:\n",
    "                print(\"Resposta do modelo:\", content_response)\n",
    "                responses.append(content_response)\n",
    "            else:\n",
    "                print(\"Resposta do modelo vazia ou malformada.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao gerar resposta: {e}\")\n",
    "\n",
    "# Combinar as respostas\n",
    "final_answer = \" \".join(responses)\n",
    "\n",
    "# Exibir a resposta final e contagem de tokens\n",
    "print(\"LLM Final Answer:\")\n",
    "print(final_answer)\n",
    "\n",
    "print(\"\\nToken Counts per Chunk:\")\n",
    "print(token_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### managing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "max_tokens = 4096\n",
    "model = \"mistral-7b-instruct-v0.3\"\n",
    "completions_url = f\"{lm_studio_url}/chat/completions\"\n",
    "prompt_template = \"{data}\\n\\n{question}\"\n",
    "context_tokens = len(tokenizer.encode(\"Você é um assistente útil.\")) + len(tokenizer.encode(question))\n",
    "max_chunk_size = max_tokens - context_tokens - 512\n",
    "\n",
    "def process_chunk(session, chunk):\n",
    "    token_count = len(tokenizer.encode(chunk))\n",
    "    if token_count > max_chunk_size:\n",
    "        split_chunks = [\n",
    "            tokenizer.decode(tokenizer.encode(chunk)[i:i + max_chunk_size])\n",
    "            for i in range(0, token_count, max_chunk_size)\n",
    "        ]\n",
    "    else:\n",
    "        split_chunks = [chunk]\n",
    "\n",
    "    responses = []\n",
    "    for sub_chunk in split_chunks:\n",
    "        prompt = prompt_template.format(data=sub_chunk, question=question)\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"assistant\", \"content\": \"Você é um assistente útil.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 512\n",
    "        }\n",
    "        try:\n",
    "            response = session.post(completions_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            content = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "            if content:\n",
    "                responses.append(content)\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "    return responses\n",
    "\n",
    "responses = []\n",
    "with requests.Session() as session:\n",
    "    for chunk in chunks:\n",
    "        responses.extend(process_chunk(session, chunk))\n",
    "\n",
    "final_answer = \" \".join(responses)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from transformers import AutoTokenizer\n",
    "import asyncio\n",
    "\n",
    "# Configurações\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "max_tokens = 4096\n",
    "model = \"mistral-7b-instruct-v0.3\"\n",
    "completions_url = f\"{lm_studio_url}/chat/completions\"\n",
    "prompt_template = \"{data}\\n\\n{question}\"\n",
    "context_tokens = len(tokenizer.encode(\"Você é um assistente útil.\")) + len(tokenizer.encode(question))\n",
    "max_chunk_size = max_tokens - context_tokens - 512\n",
    "\n",
    "async def process_chunk(session, chunk):\n",
    "    token_count = len(tokenizer.encode(chunk))\n",
    "    if token_count > max_chunk_size:\n",
    "        split_chunks = [\n",
    "            tokenizer.decode(tokenizer.encode(chunk)[i:i + max_chunk_size])\n",
    "            for i in range(0, token_count, max_chunk_size)\n",
    "        ]\n",
    "    else:\n",
    "        split_chunks = [chunk]\n",
    "\n",
    "    responses = []\n",
    "    for sub_chunk in split_chunks:\n",
    "        prompt = prompt_template.format(data=sub_chunk, question=question)\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"assistant\", \"content\": \"Você é um assistente útil.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 512\n",
    "        }\n",
    "        try:\n",
    "            response = await session.post(completions_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            content = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "            if content:\n",
    "                responses.append(content)\n",
    "        except httpx.RequestError:\n",
    "            pass\n",
    "    return responses\n",
    "\n",
    "async def main():\n",
    "    responses = []\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        tasks = [process_chunk(session, chunk) for chunk in chunks]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for result in results:\n",
    "            responses.extend(result)\n",
    "\n",
    "    final_answer = \" \".join(responses)\n",
    "    print(final_answer)\n",
    "\n",
    "# Executar o código\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
